# 0317

## DONE

#### 1. train()

1. hyper parameter (이하 hparams)

> TTS directory에 config.yaml 파일을 읽어 yaml 파일에 명시된 변수 사용

2. model, optimizer, scheduler, loss function 정의

> torch 라이브러리에 정의된 Adam optimizer와 StepLR scheduler 사용

```python
model = load_model(hparams)
learning_rate = float(hparams['learning_rate'])  
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=float(hparams['weight_decay']))
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(hparams['scheduler_step']), gamma=float(hparams['gamma']))
criterion = Tacotron2Loss()
```

3. prepare_dataloaders() 함수를 이용하여 dataset 준비

> hparams를 통해 위에 정의된 함수 이용

```python
train_loader, valset, collate_fn = prepare_dataloaders(hparams)
```

4. warm start여부 결정

>train.py 실행시 warm_start 여부에 따라 해당 모델을 불러오거나 체크포인트를 가져옴

```python
    if checkpoint_path is not None:
        # train from pretrained model
        if warm_start:
            # warm_start함수로 이동
            # pass
            model = warm_start_model(checkpoint_path, model, hparams['ignore_layers'])

        #train from scratch
        ##제공##
        else:
            model, optimizer, _learning_rate, iteration = load_checkpoint(
                checkpoint_path, model, optimizer)
            if hparams['use_saved_learning_rate']:
                learning_rate = _learning_rate
            iteration += 1  # next iteration is iteration + 1
            epoch_offset = max(0, int(iteration / len(train_loader)))
        ##제공##        
```

5. model을 train mode로 변경 후 training loop 작성

>train mode 전환

```python
model.train()
```

>training loop 작성

```python
            model.zero_grad()
            x, y = model.parse_batch(batch)
            y_pred = model(x)

            loss = criterion(y_pred, y)
            reduced_loss = loss.item()

            loss.backward()


            ####TODO####

            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), hparams['grad_clip_thresh'])

            optimizer.step()
            scheduler.step()
```

6. backpropagation 이후 training loss 출력

>기존 작성된 내용 사용 및 추가로 Log를 찍기 위해 Logger를 구현한 후 사용

```python
            #iteration 별 loss, grad_norm, duration 결과 출력
            if not is_overflow:
                duration = time.perf_counter() - start
                print("Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it".format(
                    iteration, reduced_loss, grad_norm, duration),end='\r')
                logger.log_training(reduced_loss, grad_norm, learning_rate, duration, iteration)

            # validation을 한 뒤 checkpoint model 저장
            if not is_overflow and (iteration % hparams['iters_per_checkpoint'] == 0):
                dur = time.perf_counter() - init_start

                # validation 진행
                validate(model, criterion, valset, iteration, hparams['batch_size'], collate_fn, epoch, dur, logger)

                # checkpoint model 저장
                checkpoint_path = os.path.join(output_directory, "checkpoint_{}".format(iteration))
                save_checkpoint(model, optimizer, scheduler, learning_rate, iteration, checkpoint_path)

            iteration += 1
```



#### 2. validate()

1. model을 evaluation mode로 전환

```python
model.eval()
```

2. with torch.no_grad로 전체 연산 묶음

```python
    # with torch.no_grad로 전체 연산을 묶음
    with torch.no_grad():
    	# validate process
```

3. validate process 내부에서 dataset 준비

```python
        # validation dataset 준비
        val_loader = DataLoader(valset, sampler=None, num_workers=1,
                                shuffle=False, batch_size=batch_size,
                                pin_memory=False, collate_fn=collate_fn)
```

4. 준비된 dataset으로 train과정에서 역전파 과정을 제외하고 loss를 계산

```python
        # validation loss 계산 (train() 코드와 동일하게 작성하지만 backpropagation을 하면 안된다)
        val_loss = 0.0
        for i, batch in enumerate(val_loader):
            x, y = model.parse_batch(batch)
            y_pred = model(x)
            loss = criterion(y_pred, y)
            reduced_val_loss = loss.item()
            val_loss += reduced_val_loss
        val_loss = val_loss / (i + 1)
```

5. validation 후 로그 기록

```python
    # validation 결과 출력 및 log 기록
    print("validation loss {}: {:9f}  ".format(iteration, val_loss))
    logger.log_validation(val_loss, model, y, y_pred, iteration)
```



#### 3. save_checkpoint()

1. torch.save를 통해 iteration, model, optimizer, scheduler의 state_dict를 저장

```python
def save_checkpoint(model, optimizer, scheduler, learning_rate, iteration, filepath): 
    print("Saving model and optimizer stat at iteration {} to {}".format(iteration, filepath))
    torch.save({'iteration': iteration,
                'state_dict': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict(),
                'learning_rate': learning_rate}, filepath)
```





## TODO

#### 1. 학습된 모델을 로드하여 사용하는 방법 학습하기

#### 2. 학습된 내용으로 TTS_MODEL 클래스를 정의하여 오디오 파일 생성하기

#### 3. django를 사용하여 gui를 통해 서비스 사용하기